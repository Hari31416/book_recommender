{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import sparse\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(\"..\", \"data\", \"movielens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ratings: 33184\n",
      "Number of unique users: 983\n",
      "Number of books: 847\n",
      "Shape of interactions: (33184, 3)\n"
     ]
    }
   ],
   "source": [
    "interactions = pd.read_csv(os.path.join(DATA_DIR, \"ratings.csv\"))\n",
    "interactions = interactions[[\"userId\", \"movieId\", \"rating\"]]\n",
    "users_to_consider = np.arange(0, 1000)\n",
    "interactions = interactions[interactions[\"userId\"].isin(users_to_consider)]\n",
    "\n",
    "movies_to_consider = np.arange(0, 1000)\n",
    "interactions = interactions[interactions[\"movieId\"].isin(movies_to_consider)]\n",
    "interactions.reset_index(drop=True, inplace=True)\n",
    "movies = pd.read_csv(os.path.join(DATA_DIR, \"movies.csv\"))\n",
    "movies = movies[movies[\"movieId\"].isin(movies_to_consider)]\n",
    "movies.set_index(\"movieId\", inplace=True)\n",
    "\n",
    "print(f\"Number of ratings: {len(interactions)}\")\n",
    "print(f\"Number of unique users: {interactions['userId'].nunique()}\")\n",
    "print(f\"Number of books: {interactions['movieId'].nunique()}\")\n",
    "print(f\"Shape of interactions: {interactions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 unique users and 1000 unique books in this data set\n"
     ]
    }
   ],
   "source": [
    "m = 1000\n",
    "n = 1000\n",
    "print(f\"There are {m} unique users and {n} unique books in this data set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, holdout_fraction=0.1):\n",
    "  \"\"\"Splits a DataFrame into training and test sets.\n",
    "  Args:\n",
    "    df: a dataframe.\n",
    "    holdout_fraction: fraction of dataframe rows to use in the test set.\n",
    "  Returns:\n",
    "    train: dataframe for training\n",
    "    test: dataframe for testing\n",
    "  \"\"\"\n",
    "  test = df.sample(frac=holdout_fraction, replace=False)\n",
    "  train = df[~df.index.isin(test.index)]\n",
    "  return train, test\n",
    "\n",
    "train_interactions, test_interactions = split_dataframe(interactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using neural networks instead of matrix factorization (MF) for recommendation systems has a number of advantages. See the notes section for detail. Here, we will be providing the model architecture that will be used for the recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will consist of two steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer will take the user and item IDs in one-hot encoded form along with any other user and item feature and will pass it through a fully connected layer. The output of this layer will be the latent representation of the user and item. Let use denote $\\mathbf{u}$ as the user and $\\mathbf{v}$ as the item. Their dimensions will be $m+m_{uf}$ and $n+n_{if}$ respectively where $m$ and $n$ are the number of users and items and $m_{uf}$ and $n_{if}$ are the number of user and item features respectively. The output of the embedding layer will be $\\mathbf{u} \\in \\mathbb{R}^d$ and $\\mathbf{v} \\in \\mathbb{R}^d$ where $d$ is the dimension of the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have two different layers, one for users and the other of items. This is required because the number of users and items are different and we want to learn different embeddings for them. The user embedding layer will have $m+m_{uf}$ neurons and the item embedding layer will have $n+n_{if}$ neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CF Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CF layers, or collaborative filtering layers are made up of one or more layers of fully connected layers. The input to these layers will be the concatenation of the user and item latent representations. The output of the CF layers will be the predicted rating.\n",
    "\n",
    "![](images/dl_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function can either be MSE or cross entropy. We will be experimenting with both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using all the positive data and a random sample of the negative data. The ratio of positive to negative data will be decided by a parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will be trained using the one-hot representation of the user and item ids along with the user and item features. The output will be the rating. So, the input will be two vectors of length $M = m+m_{uf}$ and $N = n+n_{if}$ and the output will be a scalar. This means that we will be using pointwise approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using negative sampling to train the model. This is important, as we want our model to learn that the rating of a user-item pair is zero if the user has not rated the item. We will define a variable `negative_samples_ratio` that can be used to control the ratio of positive to negative samples. The negative samples will be randomly sampled from the negative data. We can set `negative_samples_ratio` to 0.3-0.5. This means that for every positive sample, we will be using 0.3-0.5 negative samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a class that creates a dataset using the one-hot representation of the user and item ids along with the user and item features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset class for the model.\"\"\"\"\"\n",
    "    def __init__(self, interactions, negative_samples_ratio=0.5):\n",
    "        \"\"\"Initializes the BookDataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        users: pandas.DataFrame\n",
    "            DataFrame containing user features.\n",
    "        books: pandas.DataFrame\n",
    "            DataFrame containing book features.\n",
    "        interactions: pandas.DataFrame\n",
    "            DataFrame containing user-book interactions.\n",
    "        negative_samples_ratio: float\n",
    "            Ratio of negative samples to positive samples. Must be between 0 and 1.\n",
    "        \"\"\"\n",
    "        self.interactions = interactions\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.m_f = 0\n",
    "        self.n_f = 0\n",
    "        # print(f\"There are {self.m} unique users and {self.n} unique books in this data set\")\n",
    "        # print(f\"There are {self.m_f} user features and {self.n_f} book features\")\n",
    "        if negative_samples_ratio < 0 or negative_samples_ratio > 1:\n",
    "            raise ValueError(\"negative_samples_ratio must be between 0 and 1.\")\n",
    "        self.negative_samples_ratio = negative_samples_ratio\n",
    "\n",
    "    def __len__(self):\n",
    "        # tried this but this will give error as the interaction dataframe will become out of index\n",
    "        # num_times = 1+self.negative_samples_ratio\n",
    "        num_times = 1\n",
    "        return int(len(self.interactions)*num_times)\n",
    "    \n",
    "    def get_positive_sample(self, idx):\n",
    "        \"\"\"Gets a positive sample from the interactions dataframe.\"\"\"\n",
    "        row = self.interactions.iloc[idx]\n",
    "        userId = row[\"userId\"]\n",
    "        bookId = row[\"movieId\"]\n",
    "        rating = row[\"rating\"]\n",
    "        return userId, bookId, rating\n",
    "    \n",
    "    def get_negative_sample(self, idx):\n",
    "        \"\"\"Gets a negative sample from the interactions dataframe.\"\"\"\"\"\n",
    "        row = self.interactions.iloc[idx]\n",
    "        userId = row[\"userId\"]\n",
    "        negative_bookId = np.random.choice(movies.index.values)\n",
    "        while negative_bookId in self.interactions[self.interactions[\"userId\"] == userId][\"movieId\"].values:\n",
    "            negative_bookId = np.random.choice(movies.index.values)\n",
    "\n",
    "        rating = 0\n",
    "        return userId, negative_bookId, rating\n",
    "    \n",
    "    def get_one_sample(self, idx):\n",
    "        \"\"\"Gets one sample from the dataset. Uses negative sampling with probability `negative_samples_ratio`.\"\"\"\n",
    "        if np.random.random() < self.negative_samples_ratio:\n",
    "            return self.get_negative_sample(idx)\n",
    "        else:\n",
    "            return self.get_positive_sample(idx)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Gets one sample from the dataset.\"\"\"\n",
    "        # A workaround to make interaction dataframe circular when using negative sampling\n",
    "        # with `num_times` > 1. Leaving it as this should not be necessary here.\n",
    "        # if idx >= len(self.interactions):\n",
    "        #     idx = idx%len(self.interactions)\n",
    "        user_input, book_input, rating = self.get_one_sample(idx)\n",
    "        user_input = torch.tensor(user_input, dtype=torch.float32)\n",
    "        book_input = torch.tensor(book_input, dtype=torch.float32)\n",
    "        targets = torch.tensor(rating, dtype=torch.float32)\n",
    "        return user_input, book_input, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_dataset = BookDataset(interactions, 0.5)\n",
    "book_dataset_batched = torch.utils.data.DataLoader(book_dataset, batch_size=32, shuffle=True)\n",
    "user_input, book_input, targets = next(iter(book_dataset_batched))\n",
    "(targets==0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When we use `negative_samples_ratio = 0.5` we usually get 17-20 negative samples for each positive sample. This suggests that we can get away with a lower ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let use see if the negative sampling is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating: 0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [userId, movieId, rating]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating: 0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [userId, movieId, rating]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating: 0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [userId, movieId, rating]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating: 0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [userId, movieId, rating]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating: 0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [userId, movieId, rating]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "negative_samples = np.where(targets.numpy()==0)\n",
    "for idx in negative_samples[0][:5]:\n",
    "    rating = targets[idx].item()\n",
    "    print(f\"Rating: {rating}\")\n",
    "    user_id = user_input[idx].item()\n",
    "    book_id = book_input[idx].item()\n",
    "    display(interactions[(interactions[\"userId\"] == user_id) & (interactions[\"movieId\"] == book_id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating: 4.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28013</th>\n",
       "      <td>871</td>\n",
       "      <td>32</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  movieId  rating\n",
       "28013     871       32     4.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating: 2.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22231</th>\n",
       "      <td>714</td>\n",
       "      <td>608</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  movieId  rating\n",
       "22231     714      608     2.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating: 4.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4032</th>\n",
       "      <td>136</td>\n",
       "      <td>32</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      userId  movieId  rating\n",
       "4032     136       32     4.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating: 4.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3886</th>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      userId  movieId  rating\n",
       "3886     132        1     4.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating: 5.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19694</th>\n",
       "      <td>636</td>\n",
       "      <td>780</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  movieId  rating\n",
       "19694     636      780     5.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "positive_samples = np.where(targets.numpy()!=0)\n",
    "for idx in positive_samples[0][:5]:\n",
    "    rating = targets[idx].item()\n",
    "    print(f\"Rating: {rating}\")\n",
    "    user_id = user_input[idx].item()\n",
    "    book_id = book_input[idx].item()\n",
    "    display(interactions[(interactions[\"userId\"] == user_id) & (interactions[\"movieId\"] == book_id)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is working as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the final train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_samples_ratio = 0.4\n",
    "batch_size = 32\n",
    "train_dataset = BookDataset(train_interactions, negative_samples_ratio=negative_samples_ratio)\n",
    "test_dataset = BookDataset(test_interactions, negative_samples_ratio=negative_samples_ratio)\n",
    "\n",
    "train_df = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_df = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use $d = 40$ for the embedding layer. We will use 2 layers for the CF layers. We can treat these as hyperparameters and tune them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_f = 0\n",
    "n_f = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookModel(torch.nn.Module):\n",
    "    \"\"\"Model class for the BookNet model.\"\"\"\n",
    "    def __init__(self, m=m, n=n, m_f=m_f, n_f=n_f, embedding_dim=40, cf_layer_neurons = [128, 128]):\n",
    "        \"\"\"Initializes the BookNet model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        m: int\n",
    "            Number of users.\n",
    "        n: int\n",
    "            Number of books.\n",
    "        m_f: int\n",
    "            Number of user features.\n",
    "        n_f: int\n",
    "            Number of book features.\n",
    "        embedding_dim: int\n",
    "            Hidden dimension for the hidden layer.\n",
    "        cf_layer_neurons: list\n",
    "            List of integers specifying the number of neurons in each layer of the collaborative filtering part of the model.\n",
    "        \"\"\"\n",
    "        super(BookModel, self).__init__()\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.m_f = m_f\n",
    "        self.n_f = n_f\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.cf_layer_neurons = cf_layer_neurons\n",
    "\n",
    "        self.user_embedding, self.book_embedding = self.create_embedding_layer()\n",
    "        # self._init_embedding_weights(self.user_embedding)\n",
    "        # self.cf_layer = self.create_CF_layer()\n",
    "        # self.init_weights()\n",
    "        self.fc_layers = torch.nn.ModuleList()\n",
    "        for i in range(len(self.cf_layer_neurons)):\n",
    "            if i == 0:\n",
    "                self.fc_layers.append(torch.nn.Linear(self.embedding_dim*2, self.cf_layer_neurons[i]))\n",
    "            else:\n",
    "                self.fc_layers.append(torch.nn.Linear(self.cf_layer_neurons[i-1], self.cf_layer_neurons[i]))\n",
    "        self.affine_output = torch.nn.Linear(in_features=self.cf_layer_neurons[-1], out_features=1)\n",
    "        self.logistic = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "\n",
    "    def create_embedding_layer(self):\n",
    "        \"\"\"Creates the embedding layer\"\"\"\n",
    "        user_in_shape = self.m\n",
    "        book_in_shape = self.n\n",
    "        out_shape = self.embedding_dim\n",
    "        user_embedding = torch.nn.Embedding(num_embeddings=user_in_shape, embedding_dim=out_shape)\n",
    "        book_embedding = torch.nn.Embedding(num_embeddings=book_in_shape, embedding_dim=out_shape)\n",
    "        return user_embedding, book_embedding\n",
    "    \n",
    "    def _init_embedding_weights(self, embedding_layer):\n",
    "        \"\"\"Initializes the embedding layer weights with a uniform distribution.\"\"\"\n",
    "        embedding_layer.weight.data.uniform_(0, 1)\n",
    "    \n",
    "    # def init_weights(self):\n",
    "    #     \"\"\"Initializes the weights of the model.\"\"\"\n",
    "    #     self._init_embedding_weights(self.user_embedding)\n",
    "    #     self._init_embedding_weights(self.book_embedding)\n",
    "    #     for layer in self.cf_layer:\n",
    "    #         if isinstance(layer, torch.nn.Linear):\n",
    "    #             torch.nn.init.xavier_uniform_(layer.weight)\n",
    "    \n",
    "    def create_CF_layer(self):\n",
    "        \"\"\"Creates the collaborative filtering layers. Uses the number of neurons specified in `cf_layer_neurons`.\"\"\"\n",
    "        num_layers = len(self.cf_layer_neurons)\n",
    "        activation = torch.nn.ReLU()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                layers.append(torch.nn.Linear(self.embedding_dim*2, self.cf_layer_neurons[i]))\n",
    "            else:\n",
    "                layers.append(torch.nn.Linear(self.cf_layer_neurons[i-1], self.cf_layer_neurons[i]))\n",
    "            layers.append(activation)\n",
    "        layers.append(torch.nn.Linear(self.cf_layer_neurons[-1], 1))\n",
    "        layers.append(activation)\n",
    "        return torch.nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, user_input, book_input):\n",
    "        \"\"\"Forward pass of the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        user_input: torch.Tensor\n",
    "            Tensor containing the user input.\n",
    "        book_input: torch.Tensor\n",
    "            Tensor containing the book input.\n",
    "        \"\"\"\n",
    "        user_index = user_input.long()\n",
    "        user_embedded = self.user_embedding(user_index)\n",
    "        book_index = book_input.long()\n",
    "        book_embedded = self.book_embedding(book_index)\n",
    "        # Concatenate the user and book embeddings to form one vector.\n",
    "        x = torch.cat([user_embedded, book_embedded], dim=1)\n",
    "        # x = self.cf_layer(x)\n",
    "        for layer in self.fc_layers:\n",
    "            x = layer(x)\n",
    "            x = torch.nn.ReLU()(x)\n",
    "        x = self.affine_output(x)\n",
    "        x = self.logistic(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MLP, self).__init__()\n",
    "        self.config = config\n",
    "        self.num_users = config['num_users']\n",
    "        self.num_items = config['num_items']\n",
    "        self.latent_dim = config['latent_dim']\n",
    "\n",
    "        self.embedding_user = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim)\n",
    "        self.embedding_item = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim)\n",
    "\n",
    "        self.fc_layers = torch.nn.ModuleList()\n",
    "        for idx, (in_size, out_size) in enumerate(zip(config['layers'][:-1], config['layers'][1:])):\n",
    "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
    "\n",
    "        self.affine_output = torch.nn.Linear(in_features=config['layers'][-1], out_features=1)\n",
    "        self.logistic = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_input = user_indices.long()\n",
    "        item_input = item_indices.long()\n",
    "        user_embedding = self.embedding_user(user_input)\n",
    "        item_embedding = self.embedding_item(item_input)\n",
    "        vector = torch.cat([user_embedding, item_embedding], dim=-1)  # the concat latent vector\n",
    "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
    "            vector = self.fc_layers[idx](vector)\n",
    "            vector = torch.nn.ReLU()(vector)\n",
    "            # vector = torch.nn.BatchNorm1d()(vector)\n",
    "            # vector = torch.nn.Dropout(p=0.5)(vector)\n",
    "        logits = self.affine_output(vector)\n",
    "        rating = self.logistic(logits)\n",
    "        return rating\n",
    "\n",
    "    def init_weight(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'num_users': m,\n",
    "            'num_items': n,\n",
    "            'latent_dim': 40,\n",
    "            'layers': [80, 40, 20]}\n",
    "model = MLP(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BookModel(m=m, n=n, m_f=m_f, n_f=n_f, embedding_dim=40, cf_layer_neurons=[128, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BookModel(\n",
       "  (user_embedding): Embedding(1000, 40)\n",
       "  (book_embedding): Embedding(1000, 40)\n",
       "  (fc_layers): ModuleList(\n",
       "    (0): Linear(in_features=80, out_features=128, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (affine_output): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (logistic): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our model. Next, we will define a loss function and an optimizer. Then we will train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use MSE loss. We will also add the l1 and l2 regularization terms. This way, we can experiment with different regularization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE_L1L2Loss(torch.nn.Module):\n",
    "    def __init__(self, model, l1_weight=0, l2_weight=0):\n",
    "        \"\"\"Initializes the loss function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model: torch.nn.Module\n",
    "            The model to use for the loss function.\n",
    "        l1_weight: float\n",
    "            Weight for the L1 regularization term.\n",
    "        l2_weight: float\n",
    "            Weight for the L2 regularization term.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.l1_weight = l1_weight\n",
    "        self.l2_weight = l2_weight\n",
    "\n",
    "    def forward(self, y_hat, y):\n",
    "        \"\"\"The forward pass of the loss function.\"\"\"\n",
    "        mse_loss = torch.nn.functional.mse_loss(y_hat, y)\n",
    "        l2_regularization = torch.tensor(0.)\n",
    "        l1_regularization = torch.tensor(0.)\n",
    "        for param in self.model.parameters():\n",
    "            l2_regularization += torch.norm(param, 2)\n",
    "            l1_regularization += torch.norm(param, 1)\n",
    "        l1_regularization *= self.l1_weight\n",
    "        l2_regularization *= self.l2_weight\n",
    "        loss = mse_loss + l1_regularization + l2_regularization\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the Adam optimizer. We will also create a learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_weight = 0\n",
    "l2_weight = 0\n",
    "optimizer_str = \"adam\"\n",
    "lr = 0.005\n",
    "scheduler_str = \"plateau\"\n",
    "embedding_dim = 100\n",
    "\n",
    "negative_samples_ratio = 0\n",
    "batch_size = 256\n",
    "train_dataset = BookDataset(train_interactions, negative_samples_ratio=negative_samples_ratio)\n",
    "test_dataset = BookDataset(test_interactions, negative_samples_ratio=negative_samples_ratio)\n",
    "\n",
    "train_df = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_df = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "config = {'num_users': m,\n",
    "            'num_items': n,\n",
    "            'latent_dim': 40,\n",
    "            'layers': [80, 40, 20]}\n",
    "model = MLP(config)\n",
    "loss_func = MSE_L1L2Loss(model, l1_weight=l1_weight, l2_weight=l2_weight)\n",
    "if optimizer_str == \"adam\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "elif optimizer_str == \"sgd\":\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "if scheduler_str == \"plateau\":\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, verbose=True, min_lr=1e-7)\n",
    "elif scheduler_str == \"step\":\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (embedding_user): Embedding(1000, 40)\n",
       "  (embedding_item): Embedding(1000, 40)\n",
       "  (fc_layers): ModuleList(\n",
       "    (0): Linear(in_features=80, out_features=40, bias=True)\n",
       "    (1): Linear(in_features=40, out_features=20, bias=True)\n",
       "  )\n",
       "  (affine_output): Linear(in_features=20, out_features=1, bias=True)\n",
       "  (logistic): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! We have our model. Now, we will train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(user, item, rating, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    prediction = model(user, item)\n",
    "    loss = loss_func(prediction, rating)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), prediction\n",
    "\n",
    "def test_step(user, item, rating):\n",
    "    prediction = model(user, item)\n",
    "    loss = loss_func(prediction, rating)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f8dbff03dc47149fcebe5094b20fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 1.3086674771804114, Test loss: 1.1599612150054712\n",
      "Epoch: 1, Train loss: 1.15400496903783, Test loss: 1.1445719940731158\n",
      "Epoch: 2, Train loss: 1.1425855527980935, Test loss: 1.1361437485768244\n",
      "Epoch: 3, Train loss: 1.1357976405919237, Test loss: 1.1609616222289891\n",
      "Epoch: 4, Train loss: 1.1268075398574293, Test loss: 1.13704321705378\n",
      "Epoch: 5, Train loss: 1.1205213466854627, Test loss: 1.1368224362914379\n",
      "Epoch: 6, Train loss: 1.113202959011913, Test loss: 1.129157162629641\n",
      "Epoch: 7, Train loss: 1.11125224793621, Test loss: 1.1241886724646275\n",
      "Epoch: 8, Train loss: 1.109127842671611, Test loss: 1.125126754435209\n",
      "Epoch: 9, Train loss: 1.1098112429171099, Test loss: 1.1375093448620577\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    batch = 0\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    for user, item, rating in train_df:\n",
    "        batch += 1\n",
    "        loss, prediction = train_step(user, item, rating, optimizer)\n",
    "        train_loss += loss\n",
    "        print(f\"Epoch: {epoch}, Batch: {batch}/{len(train_df)}, Loss: {loss}\", end = \"\\r\")\n",
    "        # if batch % 10 == 0:\n",
    "        #     print(rating)\n",
    "        #     print(rating.mean())\n",
    "    train_loss /= len(train_df)\n",
    "    train_losses.append(train_loss)\n",
    "    for user, item, rating in test_df:\n",
    "        loss = test_step(user, item, rating)\n",
    "        test_loss += loss\n",
    "    test_loss /= len(test_df)\n",
    "    test_losses.append(test_loss)\n",
    "    scheduler.step(test_loss)\n",
    "    print(f\"Epoch: {epoch}, Train loss: {train_loss}, Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input, book_input, targets = next(iter(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(user_input, book_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
