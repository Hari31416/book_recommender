{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from fuzzywuzzy import fuzz\n",
    "from datetime import datetime\n",
    "import re\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan of action is to use user from [3], book detail from [2], and genre and author from [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 278858 entries, 0 to 278857\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype  \n",
      "---  ------    --------------   -----  \n",
      " 0   user_id   278858 non-null  int64  \n",
      " 1   location  278858 non-null  object \n",
      " 2   age       168096 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 6.4+ MB\n"
     ]
    }
   ],
   "source": [
    "users = pd.read_csv(\"3_clean/Users.csv\")\n",
    "users.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>location</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>nyc, new york, usa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>stockton, california, usa</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>moscow, yukon territory, russia</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>porto, v.n.gaia, portugal</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>farnborough, hants, united kingdom</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                            location   age\n",
       "0        1                  nyc, new york, usa   NaN\n",
       "1        2           stockton, california, usa  18.0\n",
       "2        3     moscow, yukon territory, russia   NaN\n",
       "3        4           porto, v.n.gaia, portugal  17.0\n",
       "4        5  farnborough, hants, united kingdom   NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Merge Books?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way is to use ISBNs. The problem with this is that the same book (by the same author) can have different ISBN depending on the publisher. Another approach is to use book title along with the author name. Of course, this will result in some more problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps are:\n",
    "\n",
    "1. Using title and author to merge\n",
    "2. Separate the title, series name and book number\n",
    "3. Get the author names from author id\n",
    "4. Do preprocessing to both author name and title\n",
    "5. Use fuzzy matching with a very high threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start decoding the author codes. If there are more than one author, we will separate them with a colon (:)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = load_dataset(\"parquet\", data_files={\"train\": os.path.join(\"1_clean\", \"dataset.parquet\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'authors': ['[1]', '[2, 3]', '[4]', '[5, 6, 7, 8]', '[9]'],\n",
       " 'categories': ['[214, 220, 237, 2646, 2647, 2659, 2660, 2679]',\n",
       "  '[235, 3386]',\n",
       "  '[358, 2630, 360, 2632]',\n",
       "  '[377, 2978, 2980]',\n",
       "  '[2813, 2980]'],\n",
       " 'format': [1.0, 1.0, 1.0, 1.0, 2.0],\n",
       " 'isbn10': ['184018907X', '184454737X', '8416327866', '571308996', '8352518'],\n",
       " 'lang': ['en', 'en', 'es', 'en', 'en'],\n",
       " 'publication-date': ['2004-10-14 00:00:00',\n",
       "  '2009-03-13 00:00:00',\n",
       "  '2020-04-30 00:00:00',\n",
       "  '2015-10-01 00:00:00',\n",
       "  '2019-06-18 00:00:00'],\n",
       " 'title': ['Soldier Five : The Real Truth About The Bravo Two Zero Mission',\n",
       "  'Underbelly : The Gangland War',\n",
       "  'A Sir Phillip, Con Amor',\n",
       "  'QI: The Third Book of General Ignorance',\n",
       "  'The Hidden Power of F*cking Up']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[\"train\"][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Author Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mike Coburn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>John Silvester</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Andrew Rule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Julia Quinn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Andrew Hunter Murray</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   author_id           author_name\n",
       "0          1           Mike Coburn\n",
       "1          2        John Silvester\n",
       "2          3           Andrew Rule\n",
       "3          4           Julia Quinn\n",
       "4          5  Andrew Hunter Murray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors = pd.read_csv('1_clean/authors.csv')\n",
    "authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors.fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class is needed if want to run in parallel\n",
    "# Otherwise will get error like authors is not defined\n",
    "class AddAuthor:\n",
    "    def __init__(self, books, authors):\n",
    "        self.authors = authors\n",
    "        self.books = books\n",
    "\n",
    "    def _decode_author(self, row):\n",
    "        ids = row[\"authors\"]\n",
    "        ids = eval(ids)\n",
    "        authors_ = []\n",
    "        for id_ in ids:\n",
    "            author = self.authors.iloc[id_ - 1][\"author_name\"]\n",
    "            authors_.append(author)\n",
    "        row[\"authors_\"] = \": \".join(authors_)\n",
    "        return row\n",
    "    \n",
    "    def add_author(self):\n",
    "        self.books = self.books.map(self._decode_author, batch_size=10000, num_proc=8)\n",
    "        return self.books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the same function that is used in the class above\n",
    "def _decode_author(row):\n",
    "    ids = row[\"authors\"]\n",
    "    ids = eval(ids)\n",
    "    authors_ = []\n",
    "    for id_ in ids:\n",
    "        author = authors.iloc[id_ - 1][\"author_name\"]\n",
    "        authors_.append(author)\n",
    "    row[\"authors_\"] = \": \".join(authors_)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_author = AddAuthor(books, authors)\n",
    "books = add_author.add_author()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'authors': ['[1]', '[2, 3]', '[4]', '[5, 6, 7, 8]', '[9]'],\n",
       " 'categories': ['[214, 220, 237, 2646, 2647, 2659, 2660, 2679]',\n",
       "  '[235, 3386]',\n",
       "  '[358, 2630, 360, 2632]',\n",
       "  '[377, 2978, 2980]',\n",
       "  '[2813, 2980]'],\n",
       " 'format': [1.0, 1.0, 1.0, 1.0, 2.0],\n",
       " 'isbn10': ['184018907X', '184454737X', '8416327866', '571308996', '8352518'],\n",
       " 'lang': ['en', 'en', 'es', 'en', 'en'],\n",
       " 'publication-date': ['2004-10-14 00:00:00',\n",
       "  '2009-03-13 00:00:00',\n",
       "  '2020-04-30 00:00:00',\n",
       "  '2015-10-01 00:00:00',\n",
       "  '2019-06-18 00:00:00'],\n",
       " 'title': ['Soldier Five : The Real Truth About The Bravo Two Zero Mission',\n",
       "  'Underbelly : The Gangland War',\n",
       "  'A Sir Phillip, Con Amor',\n",
       "  'QI: The Third Book of General Ignorance',\n",
       "  'The Hidden Power of F*cking Up'],\n",
       " 'authors_': ['Mike Coburn',\n",
       "  'John Silvester: Andrew Rule',\n",
       "  'Julia Quinn',\n",
       "  'Andrew Hunter Murray: James Harkin: John Lloyd: John Mitchinson',\n",
       "  'The Try Guys']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have authors now. Let's load the other book data from [2] and have a look at the first 5 items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': [1, 2, 3, 4, 5],\n",
       " 'title': ['Harry Potter and the Half-Blood Prince (Harry Potter, #6)',\n",
       "  'Harry Potter and the Order of the Phoenix (Harry Potter, #5)',\n",
       "  \"Harry Potter and the Sorcerer's Stone (Harry Potter, #1)\",\n",
       "  'Harry Potter and the Chamber of Secrets (Harry Potter, #2)',\n",
       "  'Harry Potter and the Prisoner of Azkaban (Harry Potter, #3)'],\n",
       " 'authors': ['J.K. Rowling',\n",
       "  'J.K. Rowling',\n",
       "  'J.K. Rowling',\n",
       "  'J.K. Rowling',\n",
       "  'J.K. Rowling'],\n",
       " 'pages': [652, 870, 309, 352, 435],\n",
       " 'language': ['eng', 'eng', 'eng', 'eng', 'eng'],\n",
       " 'star_1': [9896, 12455, 108202, 11896, 10128],\n",
       " 'star_2': [25317, 37005, 130310, 49353, 24849],\n",
       " 'star_3': [159960, 211781, 567458, 288821, 194848],\n",
       " 'star_4': [556485, 604283, 1513191, 706082, 630534],\n",
       " 'star_5': [1546466, 1493113, 4268227, 1504505, 1749958],\n",
       " 'rating': [4.57, 4.5, 4.47, 4.42, 4.57],\n",
       " 'total_ratings': [2298124, 2358637, 6587388, 2560657, 2610317],\n",
       " 'total_reviews': [28062, 29770, 75911, 244, 37093],\n",
       " 'isbn': [None, '0439358078', None, '0439554896', '043965548X'],\n",
       " 'publication_date': ['2006-09-16 00:00:00',\n",
       "  '2004-09-01 00:00:00',\n",
       "  '2003-11-01 00:00:00',\n",
       "  '2003-11-01 00:00:00',\n",
       "  '2004-05-01 00:00:00']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books2 = load_dataset(\"csv\", data_files=\"2_clean/book1-100k.csv\", delimiter=\",\")\n",
    "books2[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Book Name and Series Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First five books are from Harry Potter series. Let's see what detail is available for these books in the dataset [1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'authors': ['[191489, 191490]',\n",
       "  '[191489]',\n",
       "  '[191489]',\n",
       "  '[191489]',\n",
       "  '[191489]'],\n",
       " 'categories': ['[218, 222, 292, 821, 3366, 2804, 2812, 2813]',\n",
       "  '[355, 2629, 2496]',\n",
       "  '[355, 2629, 2496]',\n",
       "  '[355, 2629, 2496]',\n",
       "  '[355, 2629, 2496]'],\n",
       " 'format': [2.0, 3.0, 3.0, 3.0, 3.0],\n",
       " 'isbn10': ['1408706784',\n",
       "  '1408824094',\n",
       "  '1408824132',\n",
       "  '1408821583',\n",
       "  '1408821516'],\n",
       " 'lang': ['en', 'en', 'en', 'en', 'en'],\n",
       " 'publication-date': ['2015-04-14 00:00:00',\n",
       "  '2011-04-04 00:00:00',\n",
       "  '2011-05-03 00:00:00',\n",
       "  '2011-05-03 00:00:00',\n",
       "  '2011-05-19 00:00:00'],\n",
       " 'title': ['Very Good Lives : The Fringe Benefits of Failure and the Importance of Imagination',\n",
       "  'Harry Potter and the Chamber of Secrets',\n",
       "  'Harry Potter and the Prisoner of Azkaban',\n",
       "  'Harry Potter and the Goblet of Fire',\n",
       "  'Harry Potter and the Chamber of Secrets'],\n",
       " 'authors_': ['J.K. Rowling: Joel Holland',\n",
       "  'J.K. Rowling',\n",
       "  'J.K. Rowling',\n",
       "  'J.K. Rowling',\n",
       "  'J.K. Rowling']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JK = books[\"train\"].filter(lambda row: \"J.K. Rowling\" in row[\"authors_\"])\n",
    "JK[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the books title does not have the series name in [1] as it is in [2]. This means that we can not directly use the title to merge the two dataframes. We will have to extract the book name first. This is what we will do next. Fortunately, the book name, the series name and the book number are separated in a logical way and we can use RegEx to extract them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we have to use a class if you don't want to import re every time you call the function:\n",
    "class GetBookInfo:\n",
    "    import re\n",
    "    def __init__(self, books):\n",
    "        self.books = books\n",
    "        self.regex = re.compile(r\"(.*?)\\s\\((.*?),?\\s#(\\d{1,3}-?\\d{0,3})\\)\")\n",
    "\n",
    "    def get_book_info(self, row):\n",
    "        title = row[\"title\"]\n",
    "        matches = self.regex.findall(title)\n",
    "        if len(matches) == 0:\n",
    "            row[\"title_\"] = title\n",
    "            row[\"series\"] = \"Standalone\"\n",
    "            row[\"book_num\"] = 1\n",
    "            return row \n",
    "        matches = matches[0]\n",
    "        series = matches[1]\n",
    "        title_ = matches[0]\n",
    "        book_num = matches[2]\n",
    "        if \"-\" in book_num:\n",
    "            book_num = int(book_num.split(\"-\")[-1])\n",
    "        row[\"title_\"] = title_\n",
    "        row[\"series\"] = series\n",
    "        row[\"book_num\"] = int(book_num)\n",
    "        return row\n",
    "    \n",
    "    def map_book_info(self):\n",
    "        self.books = self.books.map(self.get_book_info, batch_size=5000, num_proc=8)\n",
    "        return self.books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the same function used in above class\n",
    "def get_series(row):\n",
    "    import re\n",
    "    title = row[\"title\"]\n",
    "    regex = re.compile(r\"(.*?)\\s\\((.*?),?\\s#(\\d{1,3}-?\\d{0,3})\\)\")\n",
    "    matches = regex.findall(title)\n",
    "    if len(matches) == 0:\n",
    "        row[\"title_\"] = title\n",
    "        row[\"series\"] = \"Standalone\"\n",
    "        row[\"book_num\"] = 1\n",
    "        return row \n",
    "    matches = matches[0]\n",
    "    series = matches[1]\n",
    "    title_ = matches[0]\n",
    "    book_num = matches[2]\n",
    "    if \"-\" in book_num:\n",
    "        book_num = int(book_num.split(\"-\")[-1])\n",
    "    row[\"title_\"] = title_\n",
    "    row[\"series\"] = series\n",
    "    row[\"book_num\"] = int(book_num)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_info_getter = GetBookInfo(books2)\n",
    "books2 = book_info_getter.map_book_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to run this for all the book dataframes in [2]. Let's see the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': [1, 2, 3, 4, 5],\n",
       " 'title': ['Harry Potter and the Half-Blood Prince (Harry Potter, #6)',\n",
       "  'Harry Potter and the Order of the Phoenix (Harry Potter, #5)',\n",
       "  \"Harry Potter and the Sorcerer's Stone (Harry Potter, #1)\",\n",
       "  'Harry Potter and the Chamber of Secrets (Harry Potter, #2)',\n",
       "  'Harry Potter and the Prisoner of Azkaban (Harry Potter, #3)'],\n",
       " 'authors': ['J.K. Rowling',\n",
       "  'J.K. Rowling',\n",
       "  'J.K. Rowling',\n",
       "  'J.K. Rowling',\n",
       "  'J.K. Rowling'],\n",
       " 'pages': [652, 870, 309, 352, 435],\n",
       " 'language': ['eng', 'eng', 'eng', 'eng', 'eng'],\n",
       " 'star_1': [9896, 12455, 108202, 11896, 10128],\n",
       " 'star_2': [25317, 37005, 130310, 49353, 24849],\n",
       " 'star_3': [159960, 211781, 567458, 288821, 194848],\n",
       " 'star_4': [556485, 604283, 1513191, 706082, 630534],\n",
       " 'star_5': [1546466, 1493113, 4268227, 1504505, 1749958],\n",
       " 'rating': [4.57, 4.5, 4.47, 4.42, 4.57],\n",
       " 'total_ratings': [2298124, 2358637, 6587388, 2560657, 2610317],\n",
       " 'total_reviews': [28062, 29770, 75911, 244, 37093],\n",
       " 'isbn': [None, '0439358078', None, '0439554896', '043965548X'],\n",
       " 'publication_date': ['2006-09-16 00:00:00',\n",
       "  '2004-09-01 00:00:00',\n",
       "  '2003-11-01 00:00:00',\n",
       "  '2003-11-01 00:00:00',\n",
       "  '2004-05-01 00:00:00'],\n",
       " 'title_': ['Harry Potter and the Half-Blood Prince',\n",
       "  'Harry Potter and the Order of the Phoenix',\n",
       "  \"Harry Potter and the Sorcerer's Stone\",\n",
       "  'Harry Potter and the Chamber of Secrets',\n",
       "  'Harry Potter and the Prisoner of Azkaban'],\n",
       " 'series': ['Harry Potter',\n",
       "  'Harry Potter',\n",
       "  'Harry Potter',\n",
       "  'Harry Potter',\n",
       "  'Harry Potter'],\n",
       " 'book_num': [6, 5, 1, 2, 3]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books2[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is working as intended. Now, the next step is to use these to merge the dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To merge, we will check similarity between the title+author. We will be using fuzzy matching for now. Later, we might try using embeddings. Here is the algorithm:\n",
    "\n",
    "- Clean the author name and title by removing any special characters and converting to lower case.\n",
    "- Give different weight to author matching and the title matching, say 0.7 and 0.3 respectively.\n",
    "- Use `partial_ratio` from `fuzzywuzzy` to get the similarity score for author as well as title. Set a threshold that must be met for the two books to be considered same. Say 90%.\n",
    "- The function must return two rows: the ID of the book matched and the score. The map function will run on the smaller dataframe [2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookMatching:\n",
    "    import re\n",
    "    from fuzzywuzzy import fuzz\n",
    "    def __init__(self, all_books, books):\n",
    "        \"\"\"all_books: list of all books in the database\n",
    "           books: list of books to be matched\"\"\"\n",
    "        self.all_books = all_books\n",
    "        self.books = books\n",
    "        self.books_all_titles = self.all_books[:][\"title_\"]\n",
    "        self.books_all_authors = self.all_books[:][\"authors\"]\n",
    "        self.author_weight = 0.7 # give more weight to author name\n",
    "        self.title_weight = 0.3\n",
    "        self.threshold = 90 # 90% match\n",
    "        self.final_results = []\n",
    "        self.regex = re.compile('[^a-zA-Z]')\n",
    "        self.fuzz_function = fuzz.token_set_ratio\n",
    "\n",
    "    def clean_name(self, name):\n",
    "        name = self.regex.sub('', name)\n",
    "        return name.lower()\n",
    "    \n",
    "    def calculate_score(self, author1, author2, title1, title2):\n",
    "        author1 = self.clean_name(author1)\n",
    "        author2 = self.clean_name(author2)\n",
    "        title1 = self.clean_name(title1)\n",
    "        title2 = self.clean_name(title2)\n",
    "        author_score = 0\n",
    "        title_score = 0\n",
    "        if author1 == author2:\n",
    "            author_score = 100\n",
    "        else:\n",
    "            author_score = self.fuzz_function(author1, author2)\n",
    "        if title1 == title2:\n",
    "            title_score = 100\n",
    "        else:\n",
    "            title_score = self.fuzz_function(title1, title2)\n",
    "        return author_score*self.author_weight + title_score*self.title_weight\n",
    "\n",
    "    def map_function(self, row):\n",
    "        title = row[\"title\"]\n",
    "        author = row[\"authors_\"]\n",
    "        max_score = 0\n",
    "        max_idx = 0\n",
    "        for i in range(len(self.books_all_titles)):\n",
    "            score = self.calculate_score(author, self.books_all_authors[i], title, self.books_all_titles[i])\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                max_idx = i\n",
    "            if max_score == 100:\n",
    "                break\n",
    "        if not max_score >= self.threshold:\n",
    "            row[\"book_id\"] = -1\n",
    "            row[\"score\"] = -1\n",
    "        else:\n",
    "            row[\"book_id\"] = max_idx\n",
    "            row[\"score\"] = max_score\n",
    "        return row\n",
    "\n",
    "    def match(self):\n",
    "        self.books_u = self.books.map(self.map_function, batch_size=10, num_proc=8)\n",
    "        return self.books_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'authors': ['[1]', '[2, 3]', '[4]', '[5, 6, 7, 8]', '[9]'],\n",
       " 'categories': ['[214, 220, 237, 2646, 2647, 2659, 2660, 2679]',\n",
       "  '[235, 3386]',\n",
       "  '[358, 2630, 360, 2632]',\n",
       "  '[377, 2978, 2980]',\n",
       "  '[2813, 2980]'],\n",
       " 'format': [1.0, 1.0, 1.0, 1.0, 2.0],\n",
       " 'isbn10': ['184018907X', '184454737X', '8416327866', '571308996', '8352518'],\n",
       " 'lang': ['en', 'en', 'es', 'en', 'en'],\n",
       " 'publication-date': ['2004-10-14 00:00:00',\n",
       "  '2009-03-13 00:00:00',\n",
       "  '2020-04-30 00:00:00',\n",
       "  '2015-10-01 00:00:00',\n",
       "  '2019-06-18 00:00:00'],\n",
       " 'title': ['Soldier Five : The Real Truth About The Bravo Two Zero Mission',\n",
       "  'Underbelly : The Gangland War',\n",
       "  'A Sir Phillip, Con Amor',\n",
       "  'QI: The Third Book of General Ignorance',\n",
       "  'The Hidden Power of F*cking Up'],\n",
       " 'authors_': ['Mike Coburn',\n",
       "  'John Silvester: Andrew Rule',\n",
       "  'Julia Quinn',\n",
       "  'Andrew Hunter Murray: James Harkin: John Lloyd: John Mitchinson',\n",
       "  'The Try Guys']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['authors', 'categories', 'format', 'isbn10', 'lang', 'publication-date', 'title', 'authors_'],\n",
       "    num_rows: 21755\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_books = books[\"train\"].filter(lambda row: \"355\" in row[\"categories\"])\n",
    "filtered_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_matcher = BookMatching(books2[\"train\"], filtered_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58292"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(book_matcher.books_all_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acfc0d7289b84a50a7b22d8e05f16988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/21755 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "books2_ = book_matcher.match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'authors': ['[191489, 191490]',\n",
       "  '[191489]',\n",
       "  '[191489]',\n",
       "  '[191489]',\n",
       "  '[191489]'],\n",
       " 'categories': ['[218, 222, 292, 821, 3366, 2804, 2812, 2813]',\n",
       "  '[355, 2629, 2496]',\n",
       "  '[355, 2629, 2496]',\n",
       "  '[355, 2629, 2496]',\n",
       "  '[355, 2629, 2496]'],\n",
       " 'format': [2.0, 3.0, 3.0, 3.0, 3.0],\n",
       " 'isbn10': ['1408706784',\n",
       "  '1408824094',\n",
       "  '1408824132',\n",
       "  '1408821583',\n",
       "  '1408821516'],\n",
       " 'lang': ['en', 'en', 'en', 'en', 'en'],\n",
       " 'publication-date': ['2015-04-14 00:00:00',\n",
       "  '2011-04-04 00:00:00',\n",
       "  '2011-05-03 00:00:00',\n",
       "  '2011-05-03 00:00:00',\n",
       "  '2011-05-19 00:00:00'],\n",
       " 'title': ['Very Good Lives : The Fringe Benefits of Failure and the Importance of Imagination',\n",
       "  'Harry Potter and the Chamber of Secrets',\n",
       "  'Harry Potter and the Prisoner of Azkaban',\n",
       "  'Harry Potter and the Goblet of Fire',\n",
       "  'Harry Potter and the Chamber of Secrets'],\n",
       " 'authors_': ['J.K. Rowling: Joel Holland',\n",
       "  'J.K. Rowling',\n",
       "  'J.K. Rowling',\n",
       "  'J.K. Rowling',\n",
       "  'J.K. Rowling'],\n",
       " 'book_id': [-1, 3, 4, 5, 3],\n",
       " 'score': [-1.0, 100.0, 100.0, 100.0, 100.0]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books2_[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Harry Potter and the Half-Blood Prince',\n",
       " 'Harry Potter and the Order of the Phoenix',\n",
       " \"Harry Potter and the Sorcerer's Stone\",\n",
       " 'Harry Potter and the Chamber of Secrets',\n",
       " 'Harry Potter and the Prisoner of Azkaban']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books2[\"train\"][:5][\"title_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J.K. Rowling: Joel Holland',\n",
       " 'J.K. Rowling',\n",
       " 'J.K. Rowling',\n",
       " 'J.K. Rowling',\n",
       " 'J.K. Rowling']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JK[:5][\"authors_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Very Good Lives : The Fringe Benefits of Failure and the Importance of Imagination',\n",
       " 'Harry Potter and the Chamber of Secrets',\n",
       " 'Harry Potter and the Prisoner of Azkaban',\n",
       " 'Harry Potter and the Goblet of Fire',\n",
       " 'Harry Potter and the Chamber of Secrets']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JK[:5][\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Harry Potter and the Half-Blood Prince',\n",
       " 'Harry Potter and the Order of the Phoenix',\n",
       " \"Harry Potter and the Sorcerer's Stone\",\n",
       " 'Harry Potter and the Chamber of Secrets',\n",
       " 'Harry Potter and the Prisoner of Azkaban']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books2[\"train\"][:5][\"title_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J.K. Rowling',\n",
       " 'J.K. Rowling',\n",
       " 'J.K. Rowling',\n",
       " 'J.K. Rowling',\n",
       " 'J.K. Rowling']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books2[\"train\"][:5][\"authors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.partial_ratio(books2[\"train\"][:5][\"authors\"][0], JK[:5][\"authors_\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "\n",
    "# model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "# model = AutoModel.from_pretrained(model_ckpt)\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cls_pooling(model_output):\n",
    "#     return model_output.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_embeddings(text_list):\n",
    "#     encoded_input = tokenizer(\n",
    "#         text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "#     )\n",
    "#     encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "#     model_output = model(**encoded_input)\n",
    "#     return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# em1 = get_embeddings(\"Harry Potter and the Sorcerer's Stone (Harry Potter  #1)\")\n",
    "# em2 = get_embeddings(\"Harry Potter and the Chamber of Secrets (Harry Potter  #2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['authors', 'categories', 'format', 'isbn10', 'lang', 'publication-date', 'title', 'authors_'],\n",
       "        num_rows: 1109383\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_dataset = books_.map(\n",
    "#     lambda x: {\"title_embeddings\": get_embeddings(x[\"title\"]).detach().cpu().numpy()[0]},\n",
    "#     batch_size=10000,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series(row):\n",
    "    import re\n",
    "    title = row[\"title\"]\n",
    "    regex = re.compile(r\"(.*?)\\s\\((.*?),?\\s#(\\d{1,3}-?\\d{0,3})\\)\")\n",
    "    matches = regex.findall(title)\n",
    "    if len(matches) == 0:\n",
    "        row[\"title_\"] = title\n",
    "        row[\"series\"] = \"Standalone\"\n",
    "        row[\"book_num\"] = 1\n",
    "        return row \n",
    "    matches = matches[0]\n",
    "    series = matches[1]\n",
    "    title_ = matches[0]\n",
    "    book_num = matches[2]\n",
    "    if \"-\" in book_num:\n",
    "        book_num = int(book_num.split(\"-\")[-1])\n",
    "    row[\"title_\"] = title_\n",
    "    row[\"series\"] = series\n",
    "    row[\"book_num\"] = int(book_num)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7730d1e927984016b2f4ea8bd85e521f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1156ad043504a61bc46467f711c9191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a586bcc6bcb6416e8a885f74765ad2e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "books2 = load_dataset(\"csv\", data_files=\"2_clean/book1-100k.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3c5a83ab5a4aebb678ec956ace52d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=5):   0%|          | 0/58292 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "books2_ = books2.map(get_series, batch_size=5000, num_proc=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_series(row):\n",
    "#     title = row[\"title\"]\n",
    "#     regex = re.compile(r\"(.*?)\\s\\((.*?),?\\s#(\\d{1,3}-?\\d{0,3})\\)\")\n",
    "#     matches = regex.findall(title)\n",
    "#     if len(matches) == 0:\n",
    "#         row[\"title_\"] = title\n",
    "#         row[\"series\"] = \"Standalone\"\n",
    "#         row[\"book_num\"] = 1\n",
    "#         # return title, \"Standalone\", 1\n",
    "#         return row\n",
    "#     matches = matches[0]\n",
    "#     series = matches[1]\n",
    "#     title_ = matches[0]\n",
    "#     book_num = matches[2]\n",
    "#     if \"-\" in book_num:\n",
    "#         book_num = int(book_num.split(\"-\")[-1])\n",
    "#     row[\"title_\"] = title_\n",
    "#     row[\"series\"] = series\n",
    "#     row[\"book_num\"] = int(book_num)\n",
    "#     # print(row)\n",
    "#     return row\n",
    "#     return title_, series, int(book_num)\n",
    "\n",
    "# books2_df = pd.read_csv(\"2_clean/book1-100k.csv\")\n",
    "# books2_df.progress_apply(get_series, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baba6258f4d04c2dbbe00a43de481a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e711a96f9914305a8afc6a5538b343e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278f8244a2a1480e83813d1a48f267c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"parquet\", data_files={\"train\": os.path.join(\"1_clean\", \"dataset.parquet\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['authors', 'categories', 'format', 'isbn10', 'lang', 'publication-date', 'title'],\n",
       "        num_rows: 1109383\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb13a008f11942d6b321af550e8227ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=5):   0%|          | 0/1109383 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_ = dataset.map(get_series, batch_size=5000, num_proc=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['authors', 'categories', 'format', 'isbn10', 'lang', 'publication-date', 'title', 'title_', 'series', 'book_num'],\n",
       "        num_rows: 1109383\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 654021 entries, 0 to 654020\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   author_id    654021 non-null  int64 \n",
      " 1   author_name  654019 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 10.0+ MB\n"
     ]
    }
   ],
   "source": [
    "authors1 = pd.read_csv(\"1_clean/authors.csv\")\n",
    "authors1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "654021"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors1 = authors1.set_index(\"author_id\").sort_index()\n",
    "authors1 = authors1.fillna(\"NA\")\n",
    "authors = authors1[\"author_name\"].values\n",
    "len(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_author_name(row):\n",
    "    ids = row[\"authors\"]\n",
    "    ids = eval(ids)\n",
    "    authors_ = []\n",
    "    for id_ in ids:\n",
    "        author = authors[id_ - 1]\n",
    "        authors_.append(author)\n",
    "    row[\"authors_\"] = \": \".join(authors_)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272a0ae893e0466fbc3aa3dc7f0c30fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=6):   0%|          | 0/1109383 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'authors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\harik\\.conda\\envs\\data-science\\lib\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"c:\\Users\\harik\\.conda\\envs\\data-science\\lib\\site-packages\\datasets\\utils\\py_utils.py\", line 1347, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"c:\\Users\\harik\\.conda\\envs\\data-science\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 3450, in _map_single\n    example = apply_function_on_filtered_inputs(example, i, offset=offset)\n  File \"c:\\Users\\harik\\.conda\\envs\\data-science\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 3353, in apply_function_on_filtered_inputs\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n  File \"C:\\Users\\harik\\AppData\\Local\\Temp\\ipykernel_10656\\1731460965.py\", line 6, in match_author_name\nNameError: name 'authors' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset_ \u001b[39m=\u001b[39m dataset_\u001b[39m.\u001b[39;49mmap(match_author_name, batch_size\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m, num_proc\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\harik\\.conda\\envs\\data-science\\lib\\site-packages\\datasets\\dataset_dict.py:853\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    851\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[0;32m    852\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m--> 853\u001b[0m     {\n\u001b[0;32m    854\u001b[0m         k: dataset\u001b[39m.\u001b[39mmap(\n\u001b[0;32m    855\u001b[0m             function\u001b[39m=\u001b[39mfunction,\n\u001b[0;32m    856\u001b[0m             with_indices\u001b[39m=\u001b[39mwith_indices,\n\u001b[0;32m    857\u001b[0m             with_rank\u001b[39m=\u001b[39mwith_rank,\n\u001b[0;32m    858\u001b[0m             input_columns\u001b[39m=\u001b[39minput_columns,\n\u001b[0;32m    859\u001b[0m             batched\u001b[39m=\u001b[39mbatched,\n\u001b[0;32m    860\u001b[0m             batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m    861\u001b[0m             drop_last_batch\u001b[39m=\u001b[39mdrop_last_batch,\n\u001b[0;32m    862\u001b[0m             remove_columns\u001b[39m=\u001b[39mremove_columns,\n\u001b[0;32m    863\u001b[0m             keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory,\n\u001b[0;32m    864\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39mload_from_cache_file,\n\u001b[0;32m    865\u001b[0m             cache_file_name\u001b[39m=\u001b[39mcache_file_names[k],\n\u001b[0;32m    866\u001b[0m             writer_batch_size\u001b[39m=\u001b[39mwriter_batch_size,\n\u001b[0;32m    867\u001b[0m             features\u001b[39m=\u001b[39mfeatures,\n\u001b[0;32m    868\u001b[0m             disable_nullable\u001b[39m=\u001b[39mdisable_nullable,\n\u001b[0;32m    869\u001b[0m             fn_kwargs\u001b[39m=\u001b[39mfn_kwargs,\n\u001b[0;32m    870\u001b[0m             num_proc\u001b[39m=\u001b[39mnum_proc,\n\u001b[0;32m    871\u001b[0m             desc\u001b[39m=\u001b[39mdesc,\n\u001b[0;32m    872\u001b[0m         )\n\u001b[0;32m    873\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    874\u001b[0m     }\n\u001b[0;32m    875\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\harik\\.conda\\envs\\data-science\\lib\\site-packages\\datasets\\dataset_dict.py:854\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    851\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[0;32m    852\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m    853\u001b[0m     {\n\u001b[1;32m--> 854\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m    855\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[0;32m    856\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[0;32m    857\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[0;32m    858\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[0;32m    859\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[0;32m    860\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    861\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[0;32m    862\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[0;32m    863\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m    864\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[0;32m    865\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[0;32m    866\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m    867\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m    868\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[0;32m    869\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[0;32m    870\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[0;32m    871\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[0;32m    872\u001b[0m         )\n\u001b[0;32m    873\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    874\u001b[0m     }\n\u001b[0;32m    875\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\harik\\.conda\\envs\\data-science\\lib\\site-packages\\datasets\\arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    591\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 592\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    593\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    594\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harik\\.conda\\envs\\data-science\\lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    555\u001b[0m }\n\u001b[0;32m    556\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    558\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    559\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harik\\.conda\\envs\\data-science\\lib\\site-packages\\datasets\\arrow_dataset.py:3189\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3182\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSpawning \u001b[39m\u001b[39m{\u001b[39;00mnum_proc\u001b[39m}\u001b[39;00m\u001b[39m processes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   3183\u001b[0m \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[0;32m   3184\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[0;32m   3185\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   3186\u001b[0m     total\u001b[39m=\u001b[39mpbar_total,\n\u001b[0;32m   3187\u001b[0m     desc\u001b[39m=\u001b[39m(desc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m (num_proc=\u001b[39m\u001b[39m{\u001b[39;00mnum_proc\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   3188\u001b[0m ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3189\u001b[0m     \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m iflatmap_unordered(\n\u001b[0;32m   3190\u001b[0m         pool, Dataset\u001b[39m.\u001b[39m_map_single, kwargs_iterable\u001b[39m=\u001b[39mkwargs_per_job\n\u001b[0;32m   3191\u001b[0m     ):\n\u001b[0;32m   3192\u001b[0m         \u001b[39mif\u001b[39;00m done:\n\u001b[0;32m   3193\u001b[0m             shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\harik\\.conda\\envs\\data-science\\lib\\site-packages\\datasets\\utils\\py_utils.py:1387\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[1;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[0;32m   1384\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1385\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m pool_changed:\n\u001b[0;32m   1386\u001b[0m         \u001b[39m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m-> 1387\u001b[0m         [async_result\u001b[39m.\u001b[39mget(timeout\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m) \u001b[39mfor\u001b[39;00m async_result \u001b[39min\u001b[39;00m async_results]\n",
      "File \u001b[1;32mc:\\Users\\harik\\.conda\\envs\\data-science\\lib\\site-packages\\datasets\\utils\\py_utils.py:1387\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1384\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1385\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m pool_changed:\n\u001b[0;32m   1386\u001b[0m         \u001b[39m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m-> 1387\u001b[0m         [async_result\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39m0.05\u001b[39;49m) \u001b[39mfor\u001b[39;00m async_result \u001b[39min\u001b[39;00m async_results]\n",
      "File \u001b[1;32mc:\\Users\\harik\\.conda\\envs\\data-science\\lib\\site-packages\\multiprocess\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "\u001b[1;31mNameError\u001b[0m: name 'authors' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_ = dataset_.map(match_author_name, batch_size=10000, num_proc=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'authors': '[1384, 1385]',\n",
       " 'categories': '[253, 272, 314, 787, 3332, 833, 3378, 834, 3379]',\n",
       " 'format': 2.0,\n",
       " 'isbn10': '1847697909',\n",
       " 'lang': 'en',\n",
       " 'publication-date': '2012-09-15 00:00:00',\n",
       " 'title': 'Researching Language Teacher Cognition and Practice : International Case Studies',\n",
       " 'title_': 'Researching Language Teacher Cognition and Practice : International Case Studies',\n",
       " 'series': 'Standalone',\n",
       " 'book_num': 1,\n",
       " 'authors_': 'Roger Barnard: Anne Burns'}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_[\"train\"][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mike Coburn'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_[\"train\"][\"authors_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 147,\n",
       " 'title': 'Rails Cookbook: Recipes for Rapid Web Development with Ruby',\n",
       " 'authors': 'Rob Orsini',\n",
       " 'pages': 514,\n",
       " 'language': 'eng',\n",
       " 'star_1': 3,\n",
       " 'star_2': 10,\n",
       " 'star_3': 20,\n",
       " 'star_4': 23,\n",
       " 'star_5': 12,\n",
       " 'rating': 3.46,\n",
       " 'total_ratings': 68,\n",
       " 'total_reviews': 1,\n",
       " 'isbn': '0596527314',\n",
       " 'publication_date': '2007-01-01 00:00:00',\n",
       " 'title_': 'Rails Cookbook: Recipes for Rapid Web Development with Ruby',\n",
       " 'series': 'Standalone',\n",
       " 'book_num': 1}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books2_[\"train\"][100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
